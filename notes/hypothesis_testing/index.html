<!DOCTYPE html>
<head>
<title>Hypothesis testing</title>
<meta charset="UTF-8">
<link rel="stylesheet" type="text/css" href="../../style.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dark.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://d3js.org/d3.v3.min.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script>hljs.initHighlightingOnLoad();</script>
</head>

<body>

<div id="t">&ensp;Hypothesis Testing </div>
<div id="p">
This topic can often feel unintuitive, obscure and even mechanical. It can sometimes be narrowed down to looking at tables to find p-values, degrees of freedom, and comparing numbers to a given z-score, or t-statistic.
<br><br>
Underlyingly, it seems to be the case that we need to "reject some hypothesis" just because the comparison of two numbers compels us to do so. But, why? Is it a social convention? Some sort of black magic? Should we belive the outcome of the test because there are textbooks that discuss the matter?
<br><br>
The motivation of this article is to provide the intuition behind hypothesis testing. Rather that taking it for granted, we will go through the foundational concepts that will allow us to comprehend why it matters, and why p-values and significance levels are important concepts when doing these tests.
<br><br>
In the mean time, we can say that this whole endevour matters because everytime we want to "test something" we could say that we are challenging the <span style="font-style:italic;">status quo</span>. And, in order to do so, we need to gather evidence that support the notion that "business as usual" is not that usual when there is some sort of intervention.
<br><br>
Take, for instance, the question <span style="font-style:italic;">"does a drug improves recovery from a disease?"</span>, or <span style="font-style:italic;">"do price discounts really drives demand up?"</span>. In this type of questions we stem from the fact that there is no effect on our intervention (drug consumption, or price discount) unless proved otherwise, and proving otherwise involes leaning on statistical and data-driven solutions. This is what hypothesis testing is all about.
</div>

<div id="st">Probability Distributions</div>
<div id="p">
Given that we can think of probabilities as "shares of possibilities", a distribution of probabilities would allow us to see how probabilities concentrate across different events. Consider, for instance, the more than typical example of tossing a coin except we'll do it several times. We know that a coin has two faces so the probability of observing either face is 50%, therefore, if we toss the coin six times, for instance, we would expect to see three heads and three tails. Nonetheless, sometimes it won't be the case.
<br><br>
<pre><code class="python" style="padding:22px;">
import random
import matplotlib.pyplot as plt

# One trial can be one coin toss
def trial(p):
    return 1 if random.random() > p else 0

# We can perform a trial "n" times    
def n_trials(n, p):
    return sum([trial(p) for _ in range(n)])

# Let's say, an experiment is set of trials, and we'll run
# 10k experiments, each of which involves 6 coin tosses.
# For each experiment, we'll record the number of "heads".
experiments = []
for k in range(10000):
    experiments.append(n_trials(6, 1/2))

# Do we see all the time a value of 0.5? If not, whata other
# values do we see?
counts = {e: len([v for v in experiments if e == v])
          for e in set(experiments)}

# Get the values, or events, in the X axis,
# and the counts, or frequencies, in the Y-axis
x = [c for c, h in counts.items()]
y = [h for c, h in counts.items()]
fig, ax = plt.subplots()
ax.bar(x, y)

# Remove bounding boxes and save/display image
[ax.spines[v].set_visible(False)
 for v in ('top', 'bottom', 'left', 'right')]
plt.savefig('trials_n_6.png', bbox_inches='tight', pad_inches=0)
plt.show()
</code></pre>
The previous code would generate the following plot:
</div>
<img src="trials_n_6.png" style="display:block; margin:auto;"/>
<br>
<div id="p">
As previously mentioned, it is not the case that we always see three exact heads everytime we toss six coins. Some times we got none, and some others we got all six heads!
<br><br>
The bar plot, or histogram, exhibits that there might be some natural variation that simply comes from the process of tossing coins multiple times. That is why we said that "a distribution of probabilities would allow us to see how probabilities concentrate across different events"; where the term "events" refer to the values in the horizontal axis, for example "getting 3 heads out of 6" or "0 out of 6". These are events, that can be also thought of as the outcomes from our experiments.
<br><br>
We, however, can see that most of the time we get 3 out of 6 tosses, since that is what we could expect as the most probable or frequent outcome: observing heads half of the time whenever we are tossing coins.
<br><br>
Now, we have introduced the notion of probability distribuitions as how frequencies concentrate over diferent values (or events). Let's round this discussion off by watching what happens when, with a fixed number of 10,000 experiments, we increase the number of trials in each experiment. Let's consider doing 20 and 1,000 trials, but for that we should refactor the code to make this task more managable.
<br><br>
<pre><code class="python" style="padding:10px;">
import random
import matplotlib.pyplot as plt

class Experiment:
    def __init__(self, p, k, n):
        self.p = p
        self.k = k
        self.n = n

    # One trial can be one coin toss
    def trial(self):
        return 1 if random.random() > self.p else 0

    # We can perform a trial "n" times
    def n_trials(self):
        return sum([self.trial() for _ in range(self.k)])

    def run_experiments(self):
        # Let's say, an experiment is set of trials, and we'll run
        # n experiments, each of which involves k coin tosses.
        # For each experiment, we'll record the number of "heads".
        experiments = []
        for k in range(self.n):
            experiments.append(self.n_trials())

        counts = {e: len([v for v in experiments if e == v])
                  for e in set(experiments)}
        return counts
        

    def __call__(self):
	    counts = self.run_experiments()
        # Get the values, or events, in the X axis,
	    # and the counts, or frequencies, in the Y-axis
	    x = [c for c, h in counts.items()]
	    y = [h for c, h in counts.items()]
	    fig, ax = plt.subplots()
	    ax.bar(x, y)

	    # Remove bounding boxes and save/display image
	    [ax.spines[v].set_visible(False)
	     for v in ('top', 'bottom', 'left', 'right')]
	    plt.savefig(f'trials_n_{self.k}.png',
	                bbox_inches='tight', pad_inches=0)
	    plt.show()

</code></pre>
<br>
After declaring the previous <code>class</code>, if we run:
<br><br>
<code class="python"><pre>
Experiment(0.5, 20, 10000)()
</code></pre>
we get:
<br><br>
<img src="trials_n_20.png" style="display:block; margin:auto;"/>
<br><br>
Similarly, if we do:
<br><br>
<code class="python"><pre>
Experiment(0.5, 1000, 10000)()
</code></pre>
<img src="trials_n_1000.png" style="display:block; margin:auto;"/>
<br><br>
It might come to our the attention that as we increase the number of trails (coin tosses; \( k \), from 6 to 20, and from 20 to 1000, until \( \infty \)). The number of distinct events increases; this is equivalent to the size of the bins in the X axis getting smaller and smaller. As this happens, we can think of the events, or values, in the horizontal axis as a continuum (hence, a continuos randon variable). For a very small change in \( X \), we can associate a frequency in the vertical axis \( Y \), regardless of how infinitesimal this change is.
<br><br>
This "continuum" across both \( X \) and \( Y \) can be generalized because we can visualize a solid line that generalizes this relationship. Now, we are ready to discuss how these lines constitutes <span style="font-weight:600;">probability density functions</span> as a means to describe the "concentration of probabilities" along the range of a random variable.
<br><br>
<img src="normal_distribution_only_axes.png" style="display:block; margin:auto;"/>
</div>
<div id="st">Probability Density Function</div>
<div id="p">
A function is a mapping mechanism to relate a elements of a set to elements of another set. In regard to probability density functions, when we input a value \( x \) we get the height of the function \(y\) (not the probability!). This height is also known as <span style="font-style:italic;">likelihood</span>, which can be interpreted as "the plausibility of observing a particular data point given the distribution (or its parameters; but more on this later, perhaps).
<br><br>
In previous paragraphs we observed how running several experiments, each of which consisted in tossing a coin \(k\) times followed a certain distribution in which most of the values gathered at the center of the range of values (or events), and surrounding the center we could see "some tails" which relate to "less frequent" cases. This is known as the <span style="font-style:600;">normal distribution</span>, it has a symmetrical shape and can be expressed with the following mathematical formula:
$$
\Large \displaystyle f(x) = \frac{1}{\sigma\sqrt{2\pi}} \cdot e^{\frac{-(x - \mu)^2}{2\sigma^{2}}}
$$
In python, we could define it as:
<code class="python"><pre>
from math import pi, exp

def normal_pdf(x, mu=0, sigma=1):
   a = 1 / (sigma*(2*pi)**1/2)
   b = exp(-(x - mu)**2 / (2*sigma**2))
   return a * b

</code></pre>

Once that function is declared, we can check whether it has the behaviour we expect:
<code class="python"><pre>
import matplotlib.pyplot as plt

x = [d/10 for d in range(-50, 51)]
y_a = [normal_pdf(d, mu=0, sigma=1) for d in x]
y_b = [normal_pdf(d, mu=1.2, sigma=0.8) for d in x]
y_c = [normal_pdf(d, mu=0, sigma=2) for d in x]

for y in (y_a, y_b, y_c):
    plt.plot(x, y)

plt.show()
</code></pre>
<img src="three_normal_distributions.png" style="display:block; margin:auto;"/>
<br><br>
We can compute the equivalent using <code>scipy</code> like this:
<br>
<code class="python"><pre>
from scipy.stats import norm
import matplotlib.pyplot as plt

x = [d/10 for d in range(-50, 51)]
y_a = [norm.pdf(d, loc=0, scale=1) for d in x]
y_b = [norm.pdf(d, loc=1.2, scale=0.8) for d in x]
y_c = [norm.pdf(d, loc=0, scale=2) for d in x]

for y in (y_a, y_b, y_c):
    plt.plot(x, y)

plt.show()
</code></pre>
<br>
We can see three normal distributions with three different parameters \(\mu\) and \(\sigma\). As mentioned earlier, the output of the function is not a probability, but the likelihood of observing the data under those parameters.
</div>
<div id="st">Cumulative Distribution Function</div>
<div id="p">
The probability density function, as discussed, is a description of how the probability is concentrated amongst diferent values of our random variable. We know that if we sum the area under the curve we would amount to the totality of events, this is what an integration does at the end of the day.
<br><br>
The integration of the probability density function is the <span style="font-weight: 600;">cumulative distribution function (CDF)</span>. This is a very useful shortcut for calculating the probability of observing a value less or equal to the \(x\) passed thorugh the function.
<br><br>
<code class="python"><pre>
from scipy.stats import norm
import matplotlib.pyplot as plt

x = [d/10 for d in range(-50, 51)]
y_a = [norm.cdf(d, loc=0, scale=1) for d in x]
y_b = [norm.cdf(d, loc=1.2, scale=0.8) for d in x]
y_c = [norm.cdf(d, loc=0, scale=2) for d in x]

for y in (y_a, y_b, y_c):
    plt.plot(x, y)

plt.show()
</code></pre>
<img src="three_normal_cdfs.png" style="display:block; margin:auto;"/>
<div id="st">Inverse of Cumulative Distribution Function</div>
Instead of knowing what is the probability associated to a certain range of values, we might be interested in the value associated to a probability.
<br><br>
<code class="python"><pre>
from scipy.stats import norm
import matplotlib.pyplot as plt

x = [d/10 for d in range(-50, 51)]
y_a = [norm.ppf(d, loc=0, scale=1) for d in x]
y_b = [norm.ppf(d, loc=1.2, scale=0.8) for d in x]
y_c = [norm.ppf(d, loc=0, scale=2) for d in x]

for y in (y_a, y_b, y_c):
    plt.plot(x, y)

plt.show()
</code></pre>
<img src="three_normal_ppfs.png" style="display:block; margin:auto;"/>
<br><br>
This concept of a function's inverse is very useful in te context of hypothesis testing, as it allows us to obtain critical values that can help us determine confidence intervals and thresholds for hypothesis testing.

Consider a normal standard normal distribution, that is \( X \sim \mathcal{N}(\mu,\,\sigma)\). If we wanted two compute the associated values to the 5% of the distribution for both tails (that is 2.5% on each end), we can do the following:
<br><br>
<code class="python"><pre>
from scipy.stats import norm
import matplotlib.pyplot as plt
import numpy as np  # We need numpy for array operations

alpha = 0.05
lower_bound = alpha / 2
upper_bound = 1 - (alpha / 2)

lower_z_critic = norm.ppf(lower_bound, loc=0, scale=1)
upper_z_critic = norm.ppf(upper_bound, loc=0, scale=1)

# Convert to numpy arrays
x = np.array([d/10 for d in range(-50, 51)])
y = np.array([norm.pdf(d, loc=0, scale=1) for d in x])

plt.plot(x, y)

# Shade lower rejection region
plt.fill_between(x[x <= lower_z_critic],
                 y[x <= lower_z_critic],
                 color='red', alpha=0.3)

# Shade upper rejection region
plt.fill_between(x[x >= upper_z_critic],
                 y[x >= upper_z_critic],
                 color='red', alpha=0.3)

# Add annotations for critical values
plt.text(lower_z_critic - 0.2, 0.05,
	 f'z = {lower_z_critic:.3f}', 
         horizontalalignment='right')
plt.text(upper_z_critic + 0.2, 0.05,
	 f'z = {upper_z_critic:.3f}', 
         horizontalalignment='left')

plt.show()
</code></pre>
<img src="shaded_significance_area.png" style="display:block; margin:auto;"/>
</div>
<div id="st">Central Limit Theorem</div>
<div id="p">
We have reached to one of the main foundational concepts of how hypothesis testing works. Everything we've covered so far was pivotal to explain the importance of the Central Limit Theorem (CLT). Let's remind ourselves about the intuition regarding "challenging the <span style="font-style:italic;">status quo</span>" using evindence backed by data.
<br><br>
CLT provides us with some interesting properties that allows us to use our evidence to see if it is as meaningful, or significant, as we would need to challenge the <span style="font-style:italic;">status quo</span>.
</div>
</body>
</html>
