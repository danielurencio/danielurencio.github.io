<!DOCTYPE html>
<head>
<title>RNN basics - Daniel Urencio</title>
<meta charset="UTF-8">
<link rel="stylesheet" type="text/css" href="../../style.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dark.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://d3js.org/d3.v3.min.js"></script>
<script src='../../plotGraphs.js'></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script>hljs.initHighlightingOnLoad();</script>
</head>

<body style='color:rgb(50,50,50);'>
 <div id="t">
Yet another RNN walkthrough
 </div>

 <div id="p">
	 So, why recurrent neural networks? Aren't conventional neural networks good enough? Turns out they aren't really. The reason is, normal NN process data as a blob of information regardless of time-dependant relationships, and it happens to be that, well, most of things happen in a time-oriented manner.
	 <br><br>
	 Consider financial data such as closing prices per day, or Internet of Things applications that read from sensors that are on permanently; both of these, along with many other cases, can be treatead as time-series problems and as such something up to the challenge should be taken into account, hence Recurrent Neural Networks.
 </div>

 <div id="st">
&ensp;Some code
 </div>

 <div id="p">
	 This code is based on the <a href='http://shop.oreilly.com/product/0636920063698.do' target=_blank>Learning Tensorflow</a> book.
 </div>

 <pre>
  <code class='python'>
  import tensorflow as tf

  from tensorflow.examples.tutorials.mnist import input_data
  mnist = input_data.read_data_sets("/tmp/data/",one_hot=True)

  # Define some parameters
  element_size = 28
  time_steps = 28
  num_classes = 10
  batch_size = 128
  hidden_layer_size = 128

  # Where to save TensorBOard model summaries
  LOG_DIR = "log/RNN_with_summaries"

  # Create placeholders for inputs, labels
  _inputs = tf.placeholder(tf.float32,\
			  shape=[None, time_steps, element_size],\
			  name='inputs')

  y = tf.placeholder(tf.float32, shape=[None, num_classes],\
  		                 name='labels')


  batch_x, batch_y = mnist.train.next_batch(batch_size)
  # Reshape data to get 28 sequences of 28 pixels
  batch_x.reshape((batch_size, time_steps, element_size))

  Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))
  Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))
  b_rnn = tf.Variable(tf.zeros([hidden_layer_size]))



  def rnn_step(previous_hidden_state,x):

      current_hidden_state = tf.tanh(
            tf.matmul(previous_hidden_state, Wh)\
	    		+ tf.matmul(x,Wx) + b_rnn
      )

      return current_hidden_state



  # Porcessing inputs to work with scan function
  # Current input shape: (batch_size, time_steps, elenet_size)
  processed_input = tf.transpose(_inputs, perm=[1,0,2])
  # Current input shape now: (time_steps, batch_size, element_size)

  initial_hidden = tf.zeros([batch_size,hidden_layer_size])
  # Getting all state vectors across time
  all_hidden_states = tf.scan(rnn_step, processed_input,\
  				initializer=initial_hidden,\
				name='states')

  # Weights for ouput layers
  Wl = tf.Variable(tf.truncated_normal(\
  			[hidden_layer_size, num_classes],\
			mean=0, stddev=.01))

  bl = tf.Variable(tf.truncated_normal([num_classes],\
  					mean=0, stddev=.01))


  # Apply linear layer to state vector
  def get_linear_layer(hidden_state):
      return tf.matmul(hidden_state, Wl) + bl

  # Iterate across time, apply linear layer to all RNN outputs
  all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)

  # Get last output
  output = all_outputs[-1]

  cross_entropy = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)
  )

  # Using RMSPropOptimizer
  train_step = tf.train.RMSPropOptimizer(0.001,0.9)\
  		       .minimize(cross_entropy)

  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))

  accuracy = (tf.reduce_mean(tf.cast(correct_prediction,\
  					     tf.float32)))*100

  # Get a small test set
  test_data = mnist.test.images[:batch_size]\
  		   .reshape((-1, time_steps, element_size))

  test_label = mnist.test.labels[:batch_size]

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())

      for i in range(10000):
          batch_x, batch_y = mnist.train.next_batch(batch_size)
          # Reshape data to get 28 sequences of 28 pixels
          batch_x = batch_x\
	  	    .reshape((batch_size, time_steps, element_size))

          _ = sess.run([train_step],\
	  		feed_dict={_inputs:batch_x, y:batch_y})

          if i % 1000 == 0:
              acc, loss = sess.run([accuracy,cross_entropy],\
	      		    feed_dict={_inputs: batch_x, y:batch_y})

              print ("Iter " + str(i) + ", Minibatch Loss= " +\
                   "{:.6f}".format(loss) + ", Training Accuracy= "+\
                   "{:.5f}".format(acc)) 

          if i % 10:
              # Calculate accuracy for 128 MNIST test images
              acc = sess.run([accuracy],\
	      	      feed_dict={_inputs: test_data, y: test_label})

      test_acc = sess.run(accuracy,\
      		      feed_dict={_inputs: test_data, y:test_label})

      print("Test Accuracy:", test_acc)

  </code>
 </pre>
</body>
</html>
