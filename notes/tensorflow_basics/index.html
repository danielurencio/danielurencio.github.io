<!DOCTYPE html>
<head>
<title>Tensorflow Basics - Daniel Urencio</title>
<meta charset="UTF-8">
<link rel="stylesheet" type="text/css" href="../../style.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dark.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://d3js.org/d3.v3.min.js"></script>
<script src='../../plotGraphs.js'></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script>hljs.initHighlightingOnLoad();</script>
</head>

<body style='color:rgb(50,50,50);'>
 <div id="t">
&ensp;Tensorflow Basics
 </div>

 <div id="p">
  Why Tensorflow? It is Google's own machine learning library and a very popular one. Tensorflow is useful either for building simple and intuitive models or for deploying massive applications that can be distributed among many different devices, like dedicated servers or even mobile devices.
<br><br>
As the title of these notes suggest, the main concepts and utilities of this library are going to be explored.
 </div>

 <div id="st">
&ensp;It is all about computational graphs
 </div>

 <div id="p">
 The way Tensorflow works is based on computing things through graphs. What does this mean exactly? A graph, perhaps, might be the best way to make this idea clear.
 </div>

 <div id="i_0">
 </div>

 <div id="p">
A graph, in a mathematical sense, is a set of elements that may or may not keep some sort of relationship; it is like a network where the elements are represented by nodes and the relationship between them are represented by vertices. In Tensorflow, nodes are mathematical operations and vertices signal the direction the data flows towards.
<br><br>
In the example from above information flows from left to right. The outer left nodes represent placeholders for data that will undergo certain mathematical operations; nodes <b>A</b>, <b>B</b> and <b>C</b> hold a constant value of <b>3</b>, <b>2</b> and <b>1</b> respectively. Node <b>D</b> performs a sum operation and depends on the values of <b>A</b> and <b>B</b> to compute a quantity of <b>5</b> (<b>D = A + B</b>). Similarly, <b>E</b> has dependence on <b>B</b> and <b>C</b> to compute a value of <b>1</b> through substraction (<b>E = B - C</b>).
<br><br>
Finally, as this graph's ultimate goal, node <b>F</b> computes a value of <b>5</b> (<b>F = D * E</b>) through a multiplication operation that has direct dependency on nodes <b>D</b> and <b>E</b>. However, this last node indirectly depends on the values or operations that exist before its immediate neighbour nodes (<b>A</b>, <b>B</b> and <b>C</b>).
 </div>

 <div id="st">
&ensp;But, why computational graphs?
 </div>

 <div id="p">
Computational graphs behave like an assembly process. Given that dependencies among nodes can be tracked, the computation process can be decomposed into several subprocesses for which computational resources can be allocated according to different needs. This is what allows a graph-based computation system to be distributed among different machines.
<br><br>
Furthermore, graphs are extremely flexible when it comes to building quantitative models as simple as linear regression or as sophisticated as some neural networks architectures. So, the reason why computational graphs are interesing, not to mention useful, is that both scalability and flexibility can, mostly, build any kind of application no matter how simple or complex; hence, Tensorflow is worth exploring in depth.
 </div>

 <div id='st'>
&ensp;A bit of code to exemplify
 </div>

 <div id='p' style='margin-bottom:10px;'>
There is a Tensorflow API in Python which is great for data science projects. The code for computing the graph plotted above is as simple as what follows.
 </div>

 <pre>
  <code class="python">
   import tensorflow as tf

   #  == First layer ==
   A = tf.constant(3)
   B = tf.constant(2)
   C = tf.constant(1)

   #  == Second layer ==
   D = tf.add(A,B)             #   Same as A + B semantically,
   E = tf.subtract(B,C)        #    ...    B - C ...

   #  == Target node ==
   F = tf.multiply(D,E)        #    ...    D * E ...

   #  == Intitialize session, execute it and close it ==
   sess = tf.Session()
   output = sess.run(F)        #   This variables stores a 5.
   sess.close()
  </code>
 </pre>

 <div id='p'>
Apart from the last three lines from the previous block, the entire code is self-explanatory. Once the graph has been constructed, a <b>session</b>, using the <span style='font-family:monospace;'>tf.Session()</span> command, has to be declared. Sessions are the contexts where computations are preformed; in the above example, the output obtained with node <b>F</b> is stored inside the variable named <span style='font-family:monospace;'>output</span>. After the computation is done, the session should be closed to free computational resources.
 </div>

 <div id='st'>
&ensp;Graphs, sessions and the <span style='font-family:monospace;'>with</span> command
 </div>

 <div id='p' style='margin-bottom:10px;'>
 <span id='sst'>Graphs</span>
 <br>
When Tensorflow is first imported as a library, a graph is created which, by default, is what is used for any computation; this is what happens when no anticipated specification is done. However, other graphs can be explicitly declared.
 </div>
 
 <pre>
  <code class="python">
   new_graph = tf.Graph()
  </code>
 </pre>

 <div id='p' style='margin-bottom:10px;'>
 <span id='sst'>Interactive sessions</span>
  <br>
  Besides the conventional sessions, Tensorflows has a very useful command for evaluating the content of variables and placeholders (which will be discussed shortly).
 </div>

 <pre>
  <code class="python">
   i_sess = tf.InteractiveSession()

   x = tf.constant([[1,2],
		    [3,4]])

   x.eval() 	# <-- Evaluates and prints what is stored in 'x'.

   i_sess.close()
  </code>
 </pre>

 <div id='p' style='margin-bottom:10px;'>
Interactive sessions are useful when experimenting or inspecting what kind of data types and shapes are to be fed to a graph.
<br><br>
<span id='sst'>The <span style='font-family:monospace;'>with</span> command</span><br>
In order to initialize contexts in which both graphs and sessions can be declared and executed, the <span style='font-family:monospace;'>with</span> command turns out to be a very handy utility. Suppose a diffrent graph from the default one is to be declared in any given session, to achive this the code would look like this:
 </div>

 <pre>
  <code class="python">
   with tf.Graph().as_default():
     x = tf.constant(1)
     y = tf.constant(1)
     z = x + y

     with tf.Session() as sess:
       sess.run(z)
  </code>
 </pre>

 <div id='p'>
What makes the <span style='font-family:monospace;'>with</span> command useful is that it creates a scope for a graph in which certain variables are declared. Then, within that scope, another context is created for a session to be run and compute whatever the graph's architecture is suppossed to do. Also, another benefit is that <span style='font-family:monospace;'>with</span> automatically closes the used session to immediately release resources.
 
<br><br>
By using this command diffrent graphs within different sessions can be used independently. Note that, when declaring a new <span style="font-family:monospace;">tf.Graph()</span>, it has to be called along git the <span style='font-family:monospace;'>as_default</span> method so that Tensorflow knows that this is the current graph context to use.
 </div>

 <div id='st'>
&ensp;Numpy compatibility
 </div>

 <div id='p' style='margin-bottom:10px;'>
Using Numpy along with Tensorflow makes certain mathematical tasks very easy. One example of this is matrix multiplication; Numpy creates matrices for Tensorflow and the latter does not have to worry about vector shapes.
 </div>

 <pre>
  <code class='python'>
  import tensorflow as tf
  import numpy as np

   with tf.Graph().as_default():
     a = np.random.random((1,3))
     b = np.random.random((3,1))
     c = tf.matmul(a,b)

     with tf.Session() as sess:
       sess.run(c)
  </code>
 </pre>

 <div id='p'>
According to the previos multiplication, Tensorflow will return a <b>1 x 1</b> matrix as a Numpy array.
 </div>

 <div id='st'>
&ensp;Variables and Placeholders for Optimization
 </div>

 <div id='p' style='margin-bottom:10px;'>
Most machine learning models reduce to an optimization problem where there is a cost or error function that evaluates how good a given model is. A model's 'learning' process happens as the error function is minimized given some <b>fixed parameters</b>.
<br><br>
The optimization becomes possible as some other <b>variable parameters</b> change, and finding a set of these latter parameters that result in a lower error function value is what the 'learning' is all about. Tensorflow has both a <b>Variable</b> and a <b>Placeholder</b> object to cache <b>variable</b> and <b>fixed</b> parameters respectively.
<br><br>

 <span id='sst'>Linear Regression Parameters</span><br>
As an example, consider the following expression of a simple linear equation.

$$
f(\mathbf{x}) = \mathbf{x}^{\intercal} \mathbf{w} + \mathbf{b}
$$

From this equation, \( f(\mathbf{x}) \) tries to capture a functional explanation to another variable \( \mathbf{y} \) plus some random noise \( \epsilon \sim \mathcal{N}( \mu,\ \sigma^{2}) \).

$$
\mathbf{y} = f(\mathbf{x}) + \mathbf{\epsilon}
$$

In this case, \( \mathbf{x} \) and \( \mathbf{y} \) represent <b>fixed parameters</b> so they should be represented by <b>Placeholders</b>; \( \mathbf{w} \) and \( \mathbf{b} \), on the other hand, are <b>variable parameters</b> so <b>Variable</b> objects should be used instead.
 </div>

 <pre>
  <code class='python'>
   x = tf.placeholder( tf.float32, shape=[None,3] )
   y = tf.placeholder( tf.float32, shape=None )

   w = tf.Variable( [[0,0,0]], dtype=tf.float32 )
   b = tf.Variable( 0, dtype=tf.float32 )
  </code>
 </pre>

 <div id='p' style='margin-bottom:10px;'>
Notice that both placeholders and variables take (at least) two parameters, however, each case is different. Placeholders need to have the data type and shape specified and it is not necessary to pass the data they are going to hold when they are declared; instead, data is passed whithin the session context when the computation is executed. Also, when no shape is specified or when it equals <span style='font-family:monospace;'>None</span> then data of any size can be fed.
<br><br>
Variables, on the other hand, must have the data passed in as a parameter along with its type. As it can be seen from the code above, the shape of the data is undetermined for the rows and a size of three is specified for the columns.
<br><br>
Now that all the required variales have been declared, the target output
can be defined.
 </div>

 <pre>
  <code class='python'>
   y_hat = tf.matmul(tf.transpose(x),w) + b
  </code>
 </pre>

 <div id='p' style='margin-bottom:10px;'>
 <span id='sst'>Linear Regression Cost Function</span><br>
The common metric for evaluating linear regression models is the Mean Squared Error (MSE) error function.

$$
C(\mathbf{w}) = \frac{1}{m}(\mathbf{y} - \mathbf{\hat{y}})^{2}
$$

By finding the gradient of the cost function with respect to the models parameters \( \nabla_{\mathbf{w}}C \) to perform gradient descent is how the error can be minimized. This is true since,

$$
\mathbf{w_{1}} := \mathbf{w}_{0} - \alpha \nabla_{\mathbf{w}_{0}} C(\mathbf{w_{0}}) \implies C(\mathbf{w_{0}}) \geq C(\mathbf{w_{1}})
$$

Where \( \alpha \) is the learning rate which modulates how big the update should be. By repeating the assingmnent expressed above in an iterative process a good fit for the model is eventually guaranteed.
<br><br>
A great thing about Tensorflow is that derivations and gradients are automatically computed, the only thing that has to be defined is what kind of function should be optimized and by what method. The code for minimizing a MSE function using gradient descent can be done with the following lines.
 </div>

 <pre>
  <code class='python'>
   # Cost function.
   cost = tf.reduce_mean( tf.square(y - y_hat) )

   # Learning rate.
   alpha = 0.01

   # Tell Tensorflow the method to optimize and how (minimize).
   optimizer = tf.train.GradientDescentOptimizer(learning_rate)
   minimizer = optimizer.minimize(cost)
  </code>
 </pre>

 <div id='p'>
 <span id='sst'>A 'toy' dataset to experiment with</span><br>

 </div>

<script src='script.js'></script>
</body>
</html>
