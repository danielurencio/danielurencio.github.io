<!DOCTYPE html>
<head>
<title>Tensorflow Basics - Daniel Urencio</title>
<meta charset="UTF-8">
<link rel="stylesheet" type="text/css" href="../../style.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dark.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://d3js.org/d3.v3.min.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script>hljs.initHighlightingOnLoad();</script>
<script src='../../plotGraphs.js'></script>
</head>

<body style='color:rgb(50,50,50);'>
 <div id="t">
&ensp;Tensorflow Basics
 </div>

 <div id="p">
  Why Tensorflow? It is Google's own machine learning library and a very popular one. Tensorflow is useful either for building simple and intuitive models or for deploying massive applications that can be distributed among many different devices, like dedicated servers or even mobile devices.
<br><br>
As the title of these notes suggest, the main concepts and utilities of this library are going to be explored.
 </div>

 <div id="st">
&ensp;It is all about computational graphs
 </div>

 <div id="p">
 The hay Tensorflow works is based on computing things through graphs. What does this mean exactly? A graph, perhaps, might be the best way to make this idea clear.
 </div>

 <div id="i_0">
 </div>

 <div id="p">
A graph, in the mathematical sense, is a set of elements that may or may not keep some sort of relationship. It is like a network where the elements are represented by nodes and the relationship between them are represented by vertices. In Tensorflow, nodes are mathematical operations and vertices signal the direction the data flows towards.
<br><br>
In the example from above information flows from left to right. The outer left nodes represent placeholders for data that will undergo certain mathematical operations; nodes <b>A</b>, <b>B</b> and <b>C</b> hold a constant value of <b>3</b>, <b>2</b> and <b>1</b> respectively. Node <b>D</b> performs a sum operation and depends on the values of <b>A</b> and <b>B</b> to compute a quantity of <b>5</b> (<b>D = A + B</b>). Similarly, <b>E</b> has dependence on <b>B</b> and <b>C</b> to compute a value of <b>1</b> through substraction (<b>E = B - C</b>).
<br><br>
Finally, as this graph's ultimate goal, node <b>F</b> computes a value of <b>1</b> (<b>F = D * E</b>) through a multiplication operation that has direct dependency on nodes <b>D</b> and <b>E</b>. However, this last node indirectly depends on the values or operations that exist before its immediate neighbour nodes (<b>A</b>, <b>B</b> and <b>C</b>).
 </div>

 <div id="st">
&ensp;But, why computational graphs?
 </div>

 <div id="p">
Computational graphs behave like an assembly process. Given that dependencies among nodes can be tracked, the computation process can be decomposed into several subprocesses for which computational resources can be allocated according to different needs. This is what allows a graph-based computation system to be distributed among different machines.
<br><br>
Furthermore, graphs are extremely flexible when it comes to building quantitative models as simple as linear regression or as sophisticated as some neural networks architectures. So, the reason why computational graphs are interesing, not to mention useful, is that both scalability and flexibility can build any kind of application no matter how simple or complex; hence, Tensorflow is worth exploring in depth.
 </div>

 <div id='st'>
&ensp;A bit of code to exemplify
 </div>

 <div id='p' style='margin-bottom:10px;'>
There is a Tensorflow API in Python which is great for data science projects. The code for computing the graph plotted above is as simple as what follows.
 </div>

 <pre>
  <code class="python">
   import tensorflow as tf

   #  == First layer ==
   A = tf.constant(3)
   B = tf.constant(2)
   C = tf.constant(1)

   #  == Second layer ==
   D = tf.add(A,B)             #   Same as A + B semantically,
   E = tf.subtract(B,C)        #    ...    B - C ...

   #  == Target node ==
   F = tf.multiply(D,E)        #    ...    D * E ...

   #  == Intitialize session, execute it and close it ==
   sess = tf.Session()
   output = sess.run(F)
   sess.close()
  </code>
 </pre>

 <div id='p'>
Apart from the last three lines from the previous block, the entire code is self-explanatory. Once the graph has been constructed a <b>session</b>, with the <span style='font-family:monospace;'>tf.Session()</span> command, is required. Sessions are the contexts where computations are preformed; in the above example, the output, obtained with node <b>F</b>, is stored inside the variable named <span style='font-family:"monospace";'>output</span>. After the computation is done, the session should be closed to free computational resources.
 </div>

 <div id='st'>
&ensp;Graphs, sessions and the <span style='font-family:monospace;'>with</span> command
 </div>

 <div id='p' style='margin-bottom:10px;'>
 <span style='font-weight:700'>Graphs</span>
 <br>
By default Tensorflow creates a graph. This default graph is the one that is used when declaring nodes and computing its values; however, other graphs can be explicitly declared.
 </div>
 
 <pre>
  <code class="python">
   new_graph = tf.Graph()
  </code>
 </pre>

 <div id='p' style='margin-bottom:10px;'>
 <span style='font-weight:700;'>Interactive session</span>
  <br>
  Besides the conventional sessions, Tensorflows has a very useful command for evaluating the content of variables and placeholders (which will be discussed shortly).
 </div>

 <pre>
  <code class="python">
   sess_ = tf.InteractiveSession()

   x = tf.constant([[1,2],
		    [3,4]])

   x.eval() 	# <-- Evaluates and prints what is stored in 'x'.

   sess_.close()
  </code>
 </pre>

 <div id='p' style='margin-bottom:10px;'>
Interactive sessions are useful when experimenting or inspecting what kind of data types and shapes are to be fed to a graph.
<br><br>
<span style='font-weight:700;'>The <span style='font-family:monospace;'>with</span> command</span><br>
In order to initialize a context in which both graphs and sessions can be declared or executed, this command is very useful. Suppose a diffrent graph from the default one is to be declared in any given session, to achive this the code would look like this:
 </div>

 <pre>
  <code class="python">
   with tf.Graph().as_default():
     x = tf.constant(1)
     y = tf.constant(1)
     z = x + y

     with tf.Session() as sess:
       sess.run(z)
  </code>
 </pre>

 <div id='p'>
What makes the <span style='font-family:monospace;'>with</span> command useful is that it creates a scope for a graph in which certain variables are declared. Then, within that scope, another context is created for a session to be run and compute whatever the graph's architecture is suppossed to do. Also, <span style='font-family:monospace;'>with</span> automatically closes the session which immediately releases computational resources.
 
<br><br>
By using this command diffrent graphs within different sessions can be used independently. Note that, when declaring a new <span style="font-family:monospace;">tf.Graph()</span>, it has to be called along git the <span style='font-family:monospace;'>as_default</span> method so that Tensorflow knows that this is the current graph context to use.
 </div>

 <div id='st'>
&ensp;Numpy compatibility
 </div>

 <div id='p' style='margin-bottom:10px;'>
Using Numpy along with Tensorflow makes certain mathematical tasks very easy. One example of this is matrix multiplication; Numpy creates matrices for Tensorflow and the latter does not have to worry about vector shapes.
 </div>

 <pre>
  <code class='python'>
  import tensorflow as tf
  import numpy as np

   with tf.Graph().as_default():
     a = np.random.random((1,3))
     b = np.random.random((3,1))
     c = tf.matmul(a,b)

     with tf.Session() as sess:
       sess.run(c)
  <code>
 </pre>

 <div id='p'>
According to the previos multiplication, Tensorflow will return a <b>1 x 1</b> matrix as a Numpy array.
 </div>

 <div id='st'>
&ensp;Variables and Placeholders for Optimization
 </div>

 <div id='p'>
Most machine learning models reduce to an optimization problems where there is a cost or error function that evaluates how good a given model is. A model's 'learning' process happens as the error function is minimized given some <b>fixed parameters</b>.
<br><br>
The optimization becomes possible as some other <b>variable parameters</b> change, and finding a set of these latter parameters that result in a lower error function value is what the 'learning' is all about. Tensorflow has both a <b>Variable</b> and a <b>Placeholder</b> object to cache <b>variable</b> and <b>fixed</b> parameters respectively.
 </div>
<script src='script.js'></script>
</body>
</html>
